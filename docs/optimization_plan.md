# B站视频转写加速优化方案

针对目前本地 CPU 转写速度较慢的问题，以下整理了三种可行的优化方案，按推荐程度排序：

## 方案一：Groq 云端超速转写（强烈推荐 🚀）

利用 Groq 专为 AI 推理设计的 LPU 芯片，将转写速度提升至极致。

*   **原理**: 将下载好的音频发送至 Groq API 进行云端转写。
*   **速度**: **极快**。1 小时的视频音频通常只需 **10-20 秒** 即可完成。
*   **成本**: 目前 Groq 提供每日免费且充裕的 API 额度（Whisper Large-v3 模型）。
*   **优点**: 
    *   速度是本地 CPU 的几十倍甚至上百倍。
    *   可以使用效果最好的 `large-v3` 模型，准确率极高。
    *   本地电脑无需承担繁重的计算任务，不卡顿。
*   **缺点**:
    *   需要联网。
    *   需要申请一个免费的 API Key (申请非常简单)。
    *   音频文件需要上传（Groq 声明不使用 API 数据训练模型）。

## 方案二：优化本地下载链路（BBDown）

目前的瓶颈很大程度上在于 `yt-dlp` 有时因风控下载失败，导致脚本自动回退到 **1倍速录制模式**。这是最耗时的环节。

*   **原理**: 引入更专业的 B 站下载工具 `BBDown` 替代或辅助 `yt-dlp`。
*   **速度**: **快**。确保直接下载到音频文件（几秒钟），避免漫长的录制等待。
*   **优点**:
    *   从根源上解决“录制慢”的问题。
    *   保持完全离线处理的隐私性。
*   **缺点**:
    *   需要下载 `BBDown.exe`。
    *   后续的转写步骤依然依赖本地 CPU，虽然比录制快，但不如 Groq 秒级出稿。

## 方案三：本地模型轻量化

如果不希望联网，也不想引入新工具，可以通过降低模型精度来换取速度。

*   **原理**: 将默认的 `small` 模型替换为 `base` 或 `tiny` 模型。
*   **速度**: **中等**。比 `small` 快 2-3 倍，但仍需数分钟。
*   **优点**:
    *   无需任何改动，只需在运行时指定参数 `--model base`。
    *   完全离线。
*   **缺点**:
    *   准确率会有肉眼可见的下降，可能会出现错别字或漏句。

---

## 建议实施路径

1.  **短期快速见效**: 
    如果您有 API Key 或者愿意申请，直接实施 **方案一 (Groq)**，体验秒级出稿。

2.  **稳健优化**:
    实施 **方案二 (BBDown)**，确保总是能下载到文件，杜绝“录制”情况发生，保证基础效率。

3.  **配置方式**:
    *   **方案一**: 需要在代码中集成 `groq` 客户端库。
    *   **方案二**: 下载 `BBDown.exe` 到项目目录，并修改 `run_bilibili_transcribe.py` 调用它。
